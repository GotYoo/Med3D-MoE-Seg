# Stage 1: 多模态对齐预训练 (A100 终极版)
# 核心策略: 单塔架构(Unified ViT) + 3D知识蒸馏(Distillation) + 强监督区域对齐(Region Alignment)

# ==================== 模型配置 ====================
model:
  type: "Med3DLISA_Stage1_Unified_Distill"
  
  # 1. 主力视觉塔 (Student) - 负责跑得快、吃得下大 Batch
  # 使用 ViT-Large，同时提取全局(3D)和局部(Patch)特征
  unified_vision_encoder:
    enabled: true
    # 使用强大的 ViT-Large 预训练权重
    vision_tower: "openai/clip-vit-large-patch14-336"
    freeze_vision_tower: false  # 【A100特权】全量解冻微调
    
    return_patches: true        # 必须开启，用于像素/区域对齐
    projection_dim: 768         # 统一投影维度
    
    # 3D 适配: 将切片视为序列处理
    use_3d_adapter: true
    num_slices: 32              # 覆盖更多切片以捕捉病灶

  # 2. 辅助 3D 编码器 (Teacher) - 负责传授 3D 结构知识
  # 利用你现有的 CT-CLIP 预训练权重，冻结不更新
  external_3d_encoder:
    enabled: true
    checkpoint: "checkpoints/ct_clip_pretrained.ckpt"
    model_type: "btb3d"         # 对应你权重的模型结构
    freeze: true                # 【关键】老师永远是冻结的
    output_dim: 512             # 原始输出维度，代码会自动投影对齐

  # 3. 文本编码器 (BioBERT)
  text_encoder:
    model_name: "dmis-lab/biobert-v1.1"
    freeze_biobert: false       # 解冻微调，适应 RadGenome 的描述风格
    hidden_size: 768

  # 4. 对齐模块参数
  alignment:
    embed_dim: 768
    temperature: 0.07           # InfoNCE 温度系数

# ==================== 数据配置 ====================
data:
  dataset_name: "RadGenome-ChestCT" # 指定大数据集
  dataset_type: "LIDCAlignmentDataset" # 或 RadGenome 专用的 Dataset 类
  
  # 数据路径 (请根据集群实际路径修改)
  data_root: "datasets/RadGenome/processed"
  train_json: "datasets/RadGenome/splits/train.json"
  val_json: "datasets/RadGenome/splits/val.json"
  
  # 【关键】利用 RadGenome 的强监督信号
  require_mask: true            # 读取解剖结构 Mask
  require_report: true
  
  inputs:
    ct_volume: true
    clinical_report: true
  
  # 高分辨率输入 (A100 80GB 扛得住)
  image_size: [128, 128, 128] 
  num_slices: 32
  normalize: true
  
  # 数据增强 (增强鲁棒性)
  augmentation:
    enabled: true
    random_flip: true
    random_rotation: true
    random_scale: [0.8, 1.2]
    random_noise: true

  # DataLoader 性能优化
  num_workers: 16               # 拉满 CPU 核心
  pin_memory: true
  prefetch_factor: 4

# ==================== 训练配置 ====================
training:
  stage: 1
  stage_name: "alignment_distill"
  output_dir: "outputs/stage1_unified_a100"
  seed: 42
  
  # 【核心】大 Batch Size 策略
  num_train_epochs: 100
  per_device_train_batch_size: 32  # 单卡 32，4卡就是 128，对比学习效果极佳
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1   # 不需要累积
  
  # 优化器配置
  optimizer: "adamw_torch"
  learning_rate: 5.0e-5            # 解冻微调的标准 LR
  weight_decay: 0.05
  max_grad_norm: 1.0
  
  # 显存优化
  gradient_checkpointing: true     # 开启以支持大 Batch
  bf16: true                       # A100 必开 BF16 加速
  
  # 复合损失函数权重
  loss_weights:
    # 1. 全局图文对齐 (基础任务)
    global_contrastive_loss: 1.0
    
    # 2. 局部区域对齐 (强监督任务)
    # 利用 RadGenome Mask，让 Student 学会"左肺"在哪里
    # 如果数据没有 Mask，代码会自动降级为弱监督 GLoRIA Loss
    local_contrastive_loss: 1.0    
    
    # 3. 3D 知识蒸馏 (辅助任务)
    # 强制 Student 的全局特征向 Teacher (3D Encoder) 靠拢
    distillation_loss: 2.0         
  
  # 评估与保存
  eval_strategy: "steps"
  eval_steps: 500
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 5
  logging_steps: 10
  report_to: ["tensorboard", "wandb"] # 建议开启 wandb 监控 Loss 曲线

# ==================== DeepSpeed 配置 ====================
deepspeed:
  enabled: true
  # 纯 GPU 模式 (ZeRO-2 No Offload)
  # A100 显存够大，不需要 CPU Offload，速度最快
  config_file: "config/ds_config_zero2_no_offload.json"