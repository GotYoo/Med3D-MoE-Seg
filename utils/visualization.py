"""
Visualization utilities for evaluation results
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from pathlib import Path
from typing import Optional, Tuple
import torch


def visualize_segmentation_3d(image: np.ndarray,
                              pred_mask: np.ndarray,
                              gt_mask: np.ndarray,
                              save_path: str,
                              slice_idx: Optional[int] = None,
                              title: str = "Segmentation Result"):
    """
    å¯è§†åŒ– 3D åˆ†å‰²ç»“æœï¼ˆé€‰æ‹©ä¸­é—´åˆ‡ç‰‡ï¼‰
    
    Args:
        image: CT å›¾åƒ [D, H, W]
        pred_mask: é¢„æµ‹æ©ç  [D, H, W]
        gt_mask: çœŸå®æ©ç  [D, H, W]
        save_path: ä¿å­˜è·¯å¾„
        slice_idx: åˆ‡ç‰‡ç´¢å¼•ï¼ˆNone åˆ™è‡ªåŠ¨é€‰æ‹©ä¸­é—´åˆ‡ç‰‡ï¼‰
        title: æ ‡é¢˜
    """
    # é€‰æ‹©åˆ‡ç‰‡
    if slice_idx is None:
        # æ‰¾åˆ° GT éé›¶çš„ä¸­é—´åˆ‡ç‰‡
        nonzero_slices = np.where(gt_mask.sum(axis=(1, 2)) > 0)[0]
        if len(nonzero_slices) > 0:
            slice_idx = nonzero_slices[len(nonzero_slices) // 2]
        else:
            slice_idx = image.shape[0] // 2
    
    # æå–åˆ‡ç‰‡
    img_slice = image[slice_idx]
    pred_slice = pred_mask[slice_idx]
    gt_slice = gt_mask[slice_idx]
    
    # å½’ä¸€åŒ–å›¾åƒ
    img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min() + 1e-8)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # ç¬¬ä¸€è¡Œï¼šåŸå›¾ + çœŸå® mask + é¢„æµ‹ mask
    axes[0, 0].imshow(img_slice, cmap='gray')
    axes[0, 0].set_title('CT Image', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(img_slice, cmap='gray')
    axes[0, 1].imshow(gt_slice, cmap='Reds', alpha=0.5 * (gt_slice > 0))
    axes[0, 1].set_title('Ground Truth', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(img_slice, cmap='gray')
    axes[0, 2].imshow(pred_slice, cmap='Blues', alpha=0.5 * (pred_slice > 0))
    axes[0, 2].set_title('Prediction', fontsize=12)
    axes[0, 2].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šå¯¹æ¯”å›¾
    # True Positive (TP) - ç»¿è‰²
    # False Positive (FP) - è“è‰²
    # False Negative (FN) - çº¢è‰²
    tp = (pred_slice > 0) & (gt_slice > 0)
    fp = (pred_slice > 0) & (gt_slice == 0)
    fn = (pred_slice == 0) & (gt_slice > 0)
    
    # åˆ›å»ºå½©è‰²å¯¹æ¯”å›¾
    overlay = np.zeros((*img_slice.shape, 3))
    overlay[tp] = [0, 1, 0]  # ç»¿è‰²: TP
    overlay[fp] = [0, 0, 1]  # è“è‰²: FP
    overlay[fn] = [1, 0, 0]  # çº¢è‰²: FN
    
    axes[1, 0].imshow(img_slice, cmap='gray')
    axes[1, 0].imshow(overlay, alpha=0.5)
    axes[1, 0].set_title('TP/FP/FN Overlay', fontsize=12)
    axes[1, 0].axis('off')
    
    # æ·»åŠ å›¾ä¾‹
    tp_patch = mpatches.Patch(color='green', label='True Positive')
    fp_patch = mpatches.Patch(color='blue', label='False Positive')
    fn_patch = mpatches.Patch(color='red', label='False Negative')
    axes[1, 0].legend(handles=[tp_patch, fp_patch, fn_patch], 
                     loc='upper right', fontsize=8)
    
    # è®¡ç®—æŒ‡æ ‡å¹¶æ˜¾ç¤º
    dice = 2 * tp.sum() / (pred_slice.sum() + gt_slice.sum() + 1e-5)
    iou = tp.sum() / ((pred_slice > 0) | (gt_slice > 0)).sum()
    
    axes[1, 1].axis('off')
    metrics_text = f"""
    Slice Metrics (#{slice_idx}):
    
    Dice: {dice:.4f}
    IoU:  {iou:.4f}
    
    TP pixels: {tp.sum()}
    FP pixels: {fp.sum()}
    FN pixels: {fn.sum()}
    """
    axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=11, 
                   verticalalignment='center', family='monospace')
    
    # 3D æŠ•å½±å¯è§†åŒ–
    axes[1, 2].imshow(pred_mask.max(axis=0), cmap='Blues', alpha=0.7)
    axes[1, 2].imshow(gt_mask.max(axis=0), cmap='Reds', alpha=0.3)
    axes[1, 2].set_title('3D Maximum Projection', fontsize=12)
    axes[1, 2].axis('off')
    
    plt.suptitle(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_metrics_comparison(metrics_dict: dict, save_path: str):
    """
    ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾
    
    Args:
        metrics_dict: æŒ‡æ ‡å­—å…¸ {metric_name: {mean, std, ...}}
        save_path: ä¿å­˜è·¯å¾„
    """
    # æå–åˆ†å‰²æŒ‡æ ‡å’Œæ–‡æœ¬æŒ‡æ ‡
    seg_metrics = {}
    text_metrics = {}
    
    for name, values in metrics_dict.items():
        if name in ['dice', 'iou', 'precision', 'recall']:
            seg_metrics[name] = values
        elif name in ['bleu1', 'bleu2', 'bleu4', 'rouge1', 'rouge2', 'rougeL', 'meteor']:
            text_metrics[name] = values
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # åˆ†å‰²æŒ‡æ ‡æŸ±çŠ¶å›¾
    if seg_metrics:
        names = list(seg_metrics.keys())
        means = [seg_metrics[n]['mean'] for n in names]
        stds = [seg_metrics[n]['std'] for n in names]
        
        x = np.arange(len(names))
        axes[0].bar(x, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels([n.upper() for n in names], rotation=45)
        axes[0].set_ylabel('Score')
        axes[0].set_title('Segmentation Metrics', fontweight='bold')
        axes[0].set_ylim([0, 1.0])
        axes[0].grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (m, s) in enumerate(zip(means, stds)):
            axes[0].text(i, m + s + 0.02, f'{m:.3f}', 
                        ha='center', va='bottom', fontsize=9)
    
    # æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡æŸ±çŠ¶å›¾
    if text_metrics:
        names = list(text_metrics.keys())
        means = [text_metrics[n]['mean'] for n in names]
        stds = [text_metrics[n]['std'] for n in names]
        
        x = np.arange(len(names))
        axes[1].bar(x, means, yerr=stds, capsize=5, alpha=0.7, color='lightcoral')
        axes[1].set_xticks(x)
        axes[1].set_xticklabels([n.upper() for n in names], rotation=45)
        axes[1].set_ylabel('Score')
        axes[1].set_title('Report Generation Metrics', fontweight='bold')
        axes[1].set_ylim([0, 1.0])
        axes[1].grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (m, s) in enumerate(zip(means, stds)):
            axes[1].text(i, m + s + 0.02, f'{m:.3f}', 
                        ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def create_evaluation_report(metrics: dict, 
                            predictions: list,
                            save_path: str,
                            stage_name: str = "Evaluation"):
    """
    åˆ›å»º Markdown è¯„ä¼°æŠ¥å‘Š
    
    Args:
        metrics: èšåˆåçš„æŒ‡æ ‡å­—å…¸
        predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        stage_name: é˜¶æ®µåç§°
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {stage_name} - Evaluation Report\n")
    lines.append(f"**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**Total Samples**: {len(predictions)}\n")
    lines.append("\n---\n")
    
    # åˆ†å‰²æŒ‡æ ‡
    lines.append("## ğŸ“Š Segmentation Metrics\n")
    lines.append("| Metric | Mean | Std | Min | Max | Median |")
    lines.append("|--------|------|-----|-----|-----|--------|")
    
    seg_metric_names = ['dice', 'iou', 'precision', 'recall', 'hd95', 'asd']
    for name in seg_metric_names:
        if name in metrics:
            m = metrics[name]
            lines.append(f"| {name.upper()} | {m['mean']:.4f} | {m['std']:.4f} | {m['min']:.4f} | {m['max']:.4f} | {m['median']:.4f} |")
    
    lines.append("\n")
    
    # æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡
    lines.append("## ğŸ“ Report Generation Metrics\n")
    lines.append("| Metric | Mean | Std | Min | Max | Median |")
    lines.append("|--------|------|-----|-----|-----|--------|")
    
    text_metric_names = ['bleu1', 'bleu2', 'bleu4', 'rouge1', 'rouge2', 'rougeL', 'meteor']
    for name in text_metric_names:
        if name in metrics:
            m = metrics[name]
            lines.append(f"| {name.upper()} | {m['mean']:.4f} | {m['std']:.4f} | {m['min']:.4f} | {m['max']:.4f} | {m['median']:.4f} |")
    
    lines.append("\n")
    
    # Top 5 æœ€å¥½çš„æ ·æœ¬
    if predictions:
        lines.append("## ğŸ† Top 5 Best Predictions (by Dice)\n")
        sorted_preds = sorted(predictions, key=lambda x: x.get('dice', 0), reverse=True)[:5]
        
        for i, pred in enumerate(sorted_preds, 1):
            lines.append(f"### {i}. Sample {pred.get('sample_id', 'unknown')}")
            lines.append(f"- **Dice**: {pred.get('dice', 0):.4f}")
            lines.append(f"- **IoU**: {pred.get('iou', 0):.4f}")
            if 'generated_text' in pred:
                lines.append(f"- **Generated**: {pred['generated_text'][:100]}...")
            lines.append("")
        
        # Top 5 æœ€å·®çš„æ ·æœ¬
        lines.append("## âš ï¸ Top 5 Worst Predictions (by Dice)\n")
        sorted_preds = sorted(predictions, key=lambda x: x.get('dice', 0))[:5]
        
        for i, pred in enumerate(sorted_preds, 1):
            lines.append(f"### {i}. Sample {pred.get('sample_id', 'unknown')}")
            lines.append(f"- **Dice**: {pred.get('dice', 0):.4f}")
            lines.append(f"- **IoU**: {pred.get('iou', 0):.4f}")
            lines.append("")
    
    # ä¿å­˜
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))


# å¯¼å…¥ pandasï¼ˆç”¨äºæŠ¥å‘Šç”Ÿæˆï¼‰
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas not installed, some visualization features may not work")
