import os
import json
import requests
import tarfile
import concurrent.futures
from tqdm import tqdm
import io

# --- é…ç½®åŒºåŸŸ ---
INPUT_JSONL = "/mnt/disk4t0/publicData/LLaVA-Med/llava_med_image_urls.jsonl"
SAVE_DIR = "/mnt/disk4t0/publicData/LLaVA-Med/images"
MAX_WORKERS = 16  # AWS S3 æŠ—å‹èƒ½åŠ›å¼ºï¼Œå¯ä»¥é€‚å½“è°ƒé«˜å¹¶å‘
# ----------------

def download_and_extract_stream(item):
    pair_id = item['pair_id']
    # ç›®æ ‡ä¿å­˜æ–‡ä»¶å
    save_path = os.path.join(SAVE_DIR, f"{pair_id}.jpg")
    
    # 1. æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
    if os.path.exists(save_path) and os.path.getsize(save_path) > 100:
        return "skipped"

    # 2. æ„é€  AWS S3 é•œåƒé“¾æ¥ (æ›¿æ¢åŸå§‹ FTP é“¾æ¥)
    # åŸå§‹: https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/83/41/PMC6149739.tar.gz
    # S3ç›®æ ‡: https://pmc-oa-opendata.s3.amazonaws.com/oa_package/83/41/PMC6149739.tar.gz
    ftp_url = item['pmc_tar_url']
    s3_url = ftp_url.replace("https://ftp.ncbi.nlm.nih.gov/pub/pmc/", "https://pmc-oa-opendata.s3.amazonaws.com/")
    
    target_file = item['image_file_path'] # å‹ç¼©åŒ…å†…çš„ç›®æ ‡æ–‡ä»¶è·¯å¾„

    try:
        # 3. å‘èµ·æµå¼è¯·æ±‚ (stream=True)
        with requests.get(s3_url, stream=True, timeout=20) as r:
            if r.status_code != 200:
                return "error_http"
            
            # 4. ä½¿ç”¨ tarfile æ‰“å¼€ç½‘ç»œæ•°æ®æµ (ç®¡é“æ¨¡å¼)
            #è¿™ç§æ–¹å¼ä¸éœ€è¦å°†æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç›´æ¥åœ¨å†…å­˜/æµä¸­è§£å‹
            with tarfile.open(fileobj=r.raw, mode="r|gz") as tar:
                for member in tar:
                    # 5. å¯»æ‰¾ç›®æ ‡å›¾ç‰‡
                    if member.name == target_file:
                        # æå–æ–‡ä»¶å¯¹è±¡
                        f = tar.extractfile(member)
                        if f:
                            with open(save_path, "wb") as out:
                                out.write(f.read())
                            return "success"
                        # æ‰¾åˆ°æ–‡ä»¶å¹¶æå–åï¼Œç›´æ¥é€€å‡ºå¾ªç¯å’Œå‡½æ•°
                        # æ­¤æ—¶ requests è¿æ¥ä¼šè¢«å…³é—­ï¼Œå‰©ä½™æ•°æ®ä¸å†ä¸‹è½½ -> æå¤§èŠ‚çœå¸¦å®½ï¼
                        return "success"
    except Exception as e:
        # å¾ˆå¤šæ—¶å€™æ˜¯ç½‘ç»œè¶…æ—¶ï¼Œå¯ä»¥åœ¨å¤–å±‚é‡è¯•
        return f"error: {str(e)}"
    
    return "not_found"

def main():
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
        
    print(f"ğŸ“– åŠ è½½ä»»åŠ¡æ–‡ä»¶: {INPUT_JSONL}")
    tasks = []
    with open(INPUT_JSONL, "r") as f:
        for line in f:
            tasks.append(json.loads(line))
            
    print(f"ğŸ“¦ ä»»åŠ¡æ€»æ•°: {len(tasks)}")
    print(f"ğŸš€ å¯åŠ¨æµå¼ä¸‹è½½ (å¹¶å‘æ•°: {MAX_WORKERS})...")
    print("ğŸ’¡ ç­–ç•¥: ä½¿ç”¨ AWS S3 é•œåƒ + æ‰¾åˆ°å›¾ç‰‡å³åœæ­¢ä¸‹è½½")

    success = 0
    skipped = 0
    failed = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(download_and_extract_stream, item): item for item in tasks}
        
        pbar = tqdm(concurrent.futures.as_completed(futures), total=len(tasks))
        for future in pbar:
            res = future.result()
            if res == "success":
                success += 1
            elif res == "skipped":
                skipped += 1
            else:
                failed += 1
                
            pbar.set_description(f"âœ…{success} â­ï¸{skipped} âŒ{failed}")

    print("\n" + "="*30)
    print(f"å¤„ç†å®Œæˆ Summary:")
    print(f"  æˆåŠŸä¸‹è½½: {success}")
    print(f"  æœ¬åœ°å·²å­˜: {skipped}")
    print(f"  å¤±è´¥/æœªæ‰¾: {failed}")
    print("="*30)

if __name__ == "__main__":
    main()